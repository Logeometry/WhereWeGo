import json
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import random
import os
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import math


# ----------- 0. 시드 고정 ----------- #
def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)


# 시드 설정
set_seed(42)

# ----------- 1. 데이터 로딩 및 전처리 ----------- #
# JSON 파일 로딩
with open('philly_bpr_triplets.json', 'r') as f:
    raw_data = json.load(f)

data = np.array(raw_data['triplets'])

# Positive / Negative 샘플 분리
pos_samples = data[data[:, 2] == 1]
neg_samples = data[data[:, 2] == 0]

# 유저 수, 아이템 수 계산
n_users = int(data[:, 0].max() + 1)
n_items = int(data[:, 1].max() + 1)

print(f"Number of users: {n_users}, Number of items: {n_items}")
print(f"Positive samples: {len(pos_samples)}, Negative samples: {len(neg_samples)}")
print(f"Pos:Neg ratio = 1:{len(neg_samples) / len(pos_samples):.1f}")

# 데이터 분할: 학습(60%), 검증(20%), 테스트(20%)
# 먼저 학습+검증 vs 테스트 분할
train_val_data, test_data = train_test_split(data, test_size=0.2, random_state=42)
# 다음으로 학습 vs 검증 분할 (0.25 = 20/80)
train_data, val_data = train_test_split(train_val_data, test_size=0.25, random_state=42)

print(f"Training data: {len(train_data)}, Validation data: {len(val_data)}, Test data: {len(test_data)}")
print(
    f"Split ratio - Train: {len(train_data) / len(data):.1%}, Val: {len(val_data) / len(data):.1%}, Test: {len(test_data) / len(data):.1%}")


# ----------- 2. Dataset 정의 ----------- #
class InteractionDataset(Dataset):
    def __init__(self, data):
        self.data = data
        self.pos_samples = data[data[:, 2] == 1]
        self.neg_samples = data[data[:, 2] == 0]
        self.labels = data[:, 2]

        # 긍정:부정 샘플 비율 계산
        self.pos_weight = len(self.neg_samples) / len(self.pos_samples) if len(self.pos_samples) > 0 else 1.0
        print(f"Dataset weight (Pos:Neg = 1:{self.pos_weight:.1f})")

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        user, item, label = self.data[idx]
        # 가중치 계산 (긍정 샘플에 더 큰 가중치)
        weight = self.pos_weight if label == 1 else 1.0
        return torch.LongTensor([user]), torch.LongTensor([item]), torch.FloatTensor([label]), torch.FloatTensor(
            [weight])


# DataLoader 생성
train_dataset = InteractionDataset(train_data)
val_dataset = InteractionDataset(val_data)
test_dataset = InteractionDataset(test_data)

train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)
val_dataloader = DataLoader(val_dataset, batch_size=1024, shuffle=False)
test_dataloader = DataLoader(test_dataset, batch_size=1024, shuffle=False)


# ----------- 3. 사용자 정의 손실 함수 ----------- #
class WeightedSquaredLoss(nn.Module):
    def __init__(self):
        super(WeightedSquaredLoss, self).__init__()

    def forward(self, predictions, targets, weights=None):
        if weights is None:
            weights = torch.ones_like(targets)
        loss = weights * (targets - predictions).pow(2)
        # 초기 로스값 안정화를 위해 합계 대신 평균 사용
        return loss.mean()


# ----------- 4. 평가 지표 함수 ----------- #
def evaluate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for user, item, label, weight in dataloader:
            user = user.to(device)
            item = item.to(device)
            label = label.float().view(-1).to(device)
            weight = weight.float().view(-1).to(device)

            prediction = model(user, item)

            # 손실 함수에 따라 계산
            if isinstance(criterion, WeightedSquaredLoss):
                loss = criterion(prediction, label, weight)
            else:  # BCEWithLogitsLoss 또는 다른 손실 함수
                loss = criterion(prediction, label)

            total_loss += loss.item()

            # 예측과 레이블 저장
            pred_binary = (torch.sigmoid(prediction) > 0.5).int().cpu().numpy()
            all_preds.extend(pred_binary)
            all_labels.extend(label.int().cpu().numpy())

    # 정확도 계산
    all_preds = np.array(all_preds)
    all_labels = np.array(all_labels)
    accuracy = np.mean(all_preds == all_labels)

    # 긍정 샘플에 대한 정확도
    pos_idx = (all_labels == 1)
    pos_accuracy = np.mean(all_preds[pos_idx] == all_labels[pos_idx]) if np.sum(pos_idx) > 0 else 0

    # 부정 샘플에 대한 정확도
    neg_idx = (all_labels == 0)
    neg_accuracy = np.mean(all_preds[neg_idx] == all_labels[neg_idx]) if np.sum(neg_idx) > 0 else 0

    return {
        'loss': total_loss / len(dataloader),
        'accuracy': accuracy,
        'pos_accuracy': pos_accuracy,
        'neg_accuracy': neg_accuracy
    }


# ----------- 5. 모델 정의 ----------- #
# GMF 모델
class GMF(nn.Module):
    def __init__(self, user_num, item_num, factor_num):
        super(GMF, self).__init__()
        self.user_embedding = nn.Embedding(user_num, factor_num)
        self.item_embedding = nn.Embedding(item_num, factor_num)
        self.predict_layer = nn.Linear(factor_num, 1)

        # 가중치 초기화
        nn.init.normal_(self.user_embedding.weight, std=0.01)
        nn.init.normal_(self.item_embedding.weight, std=0.01)
        nn.init.kaiming_uniform_(self.predict_layer.weight)
        if self.predict_layer.bias is not None:
            self.predict_layer.bias.data.zero_()

    def forward(self, user, item):
        user_embedding = self.user_embedding(user)
        item_embedding = self.item_embedding(item)
        element_product = user_embedding * item_embedding
        logits = self.predict_layer(element_product)
        return logits.view(-1)


# MLP 모델 (수정: 마지막 layer의 output을 64dim으로 맞춤)
class MLP(nn.Module):
    def __init__(self, user_num, item_num, factor_num, num_layers, dropout):
        super(MLP, self).__init__()
        self.user_embedding = nn.Embedding(user_num, factor_num * (2 ** (num_layers - 1)))
        self.item_embedding = nn.Embedding(item_num, factor_num * (2 ** (num_layers - 1)))
        self.dropout = dropout

        # MLP 레이어 생성 - 타워 패턴 (하단 층 넓고, 상위 층 좁음)
        MLP_modules = []
        for i in range(num_layers - 1):  # 마지막 레이어 제외
            input_size = factor_num * (2 ** (num_layers - i))
            MLP_modules.append(nn.Dropout(p=self.dropout))
            MLP_modules.append(nn.Linear(input_size, input_size // 2))
            MLP_modules.append(nn.ReLU())

        # 마지막 레이어: 출력 크기를 factor_num(64)로 맞춤
        last_size = factor_num * 2  # 이전 레이어의 출력 크기
        MLP_modules.append(nn.Dropout(p=self.dropout))
        MLP_modules.append(nn.Linear(last_size, factor_num))
        MLP_modules.append(nn.ReLU())

        self.MLP_layers = nn.Sequential(*MLP_modules)
        self.predict_layer = nn.Linear(factor_num, 1)

        # 가중치 초기화
        nn.init.normal_(self.user_embedding.weight, std=0.01)
        nn.init.normal_(self.item_embedding.weight, std=0.01)

        for m in self.MLP_layers:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    m.bias.data.zero_()

        nn.init.kaiming_uniform_(self.predict_layer.weight)
        if self.predict_layer.bias is not None:
            self.predict_layer.bias.data.zero_()

    def forward(self, user, item):
        user_embedding = self.user_embedding(user)
        item_embedding = self.item_embedding(item)
        vector = torch.cat([user_embedding, item_embedding], dim=-1)
        vector = self.MLP_layers(vector)  # 이제 vector는 factor_num(64) 크기
        logits = self.predict_layer(vector)
        return logits.view(-1)


# NeuMF 모델
class NeuMF(nn.Module):
    def __init__(self, user_num, item_num, factor_num, num_layers, dropout, alpha=0.5):
        super(NeuMF, self).__init__()
        self.alpha = alpha  # GMF와 MLP의 균형을 조정하는 하이퍼파라미터

        # GMF 부분
        self.user_gmf_embedding = nn.Embedding(user_num, factor_num)
        self.item_gmf_embedding = nn.Embedding(item_num, factor_num)

        # MLP 부분
        self.user_mlp_embedding = nn.Embedding(user_num, factor_num * (2 ** (num_layers - 1)))
        self.item_mlp_embedding = nn.Embedding(item_num, factor_num * (2 ** (num_layers - 1)))

        # MLP 레이어
        MLP_modules = []
        for i in range(num_layers - 1):  # 마지막 레이어 제외
            input_size = factor_num * (2 ** (num_layers - i))
            MLP_modules.append(nn.Dropout(p=dropout))
            MLP_modules.append(nn.Linear(input_size, input_size // 2))
            MLP_modules.append(nn.ReLU())

        # 마지막 레이어: 출력 크기를 factor_num(64)로 맞춤
        last_size = factor_num * 2  # 이전 레이어의 출력 크기
        MLP_modules.append(nn.Dropout(p=dropout))
        MLP_modules.append(nn.Linear(last_size, factor_num))
        MLP_modules.append(nn.ReLU())

        self.MLP_layers = nn.Sequential(*MLP_modules)

        # 예측 레이어 (GMF + MLP)
        self.predict_layer = nn.Linear(factor_num * 2, 1)

        # 가중치 초기화
        self._init_weight_()

    def _init_weight_(self):
        # 임베딩 레이어 초기화
        nn.init.normal_(self.user_gmf_embedding.weight, std=0.01)
        nn.init.normal_(self.item_gmf_embedding.weight, std=0.01)
        nn.init.normal_(self.user_mlp_embedding.weight, std=0.01)
        nn.init.normal_(self.item_mlp_embedding.weight, std=0.01)

        # MLP 레이어 초기화
        for m in self.MLP_layers:
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    m.bias.data.zero_()

        # 예측 레이어 초기화
        nn.init.kaiming_uniform_(self.predict_layer.weight)
        if self.predict_layer.bias is not None:
            self.predict_layer.bias.data.zero_()

    def forward(self, user, item):
        # GMF 부분
        user_gmf_embedding = self.user_gmf_embedding(user)
        item_gmf_embedding = self.item_gmf_embedding(item)
        gmf_output = user_gmf_embedding * item_gmf_embedding  # 크기: factor_num(64)

        # MLP 부분
        user_mlp_embedding = self.user_mlp_embedding(user)
        item_mlp_embedding = self.item_mlp_embedding(item)
        mlp_input = torch.cat([user_mlp_embedding, item_mlp_embedding], dim=-1)
        mlp_output = self.MLP_layers(mlp_input)  # 크기: factor_num(64)

        # NeuMF: GMF와 MLP의 결합
        concatenated = torch.cat([gmf_output, mlp_output], dim=-1)  # 크기: factor_num*2(128)
        prediction = self.predict_layer(concatenated)

        return prediction.view(-1)

    def load_pretrained(self, gmf_model, mlp_model):
        # GMF 임베딩 가중치 로드
        self.user_gmf_embedding.weight.data.copy_(gmf_model.user_embedding.weight.data)
        self.item_gmf_embedding.weight.data.copy_(gmf_model.item_embedding.weight.data)

        # MLP 임베딩 가중치 로드
        self.user_mlp_embedding.weight.data.copy_(mlp_model.user_embedding.weight.data)
        self.item_mlp_embedding.weight.data.copy_(mlp_model.item_embedding.weight.data)

        # MLP 레이어 가중치 로드
        # 각 레이어의 입력 및 출력 크기를 맞추는 것이 중요함
        mlp_layers = []
        for module in mlp_model.MLP_layers:
            if isinstance(module, nn.Linear):
                mlp_layers.append(module)

        current_mlp_layers = []
        for module in self.MLP_layers:
            if isinstance(module, nn.Linear):
                current_mlp_layers.append(module)

        for idx, layer in enumerate(current_mlp_layers):
            if idx < len(mlp_layers):
                if layer.weight.size() == mlp_layers[idx].weight.size():
                    layer.weight.data.copy_(mlp_layers[idx].weight.data)
                    if layer.bias is not None and mlp_layers[idx].bias is not None:
                        layer.bias.data.copy_(mlp_layers[idx].bias.data)

        # 예측 레이어 가중치 로드 (α를 사용하여 GMF와 MLP 간의 균형 조정)
        # GMF 예측 레이어 가중치 크기 확장
        gmf_weight = gmf_model.predict_layer.weight.data  # 크기: [1, factor_num]

        # MLP 예측 레이어 가중치
        mlp_weight = mlp_model.predict_layer.weight.data  # 크기: [1, factor_num]

        # NeuMF 예측 레이어 가중치 설정 (GMF와 MLP 부분으로 나눔)
        self.predict_layer.weight.data[:, :factor_num].copy_(self.alpha * gmf_weight)
        self.predict_layer.weight.data[:, factor_num:].copy_((1 - self.alpha) * mlp_weight)

        # 바이어스 설정
        if self.predict_layer.bias is not None:
            self.predict_layer.bias.data.copy_(
                self.alpha * gmf_model.predict_layer.bias.data +
                (1 - self.alpha) * mlp_model.predict_layer.bias.data
            )


# ----------- 6. 학습 함수 ----------- #
def train(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for user, item, label, weight in train_loader:
        user = user.to(device)
        item = item.to(device)
        label = label.float().view(-1).to(device)
        weight = weight.float().view(-1).to(device)

        optimizer.zero_grad()
        prediction = model(user, item)

        # 손실 함수에 따라 계산
        if isinstance(criterion, WeightedSquaredLoss):
            loss = criterion(prediction, label, weight)
        else:  # BCEWithLogitsLoss 또는 다른 손실 함수
            # BCEWithLogitsLoss에 가중치 적용
            loss = criterion(prediction, label)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()

    return total_loss / len(train_loader)


# ----------- 7. 모델 저장 및 로드 함수 ----------- #
def save_model(model, path):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    torch.save(model.state_dict(), path)
    print(f"Model saved: {path}")


def load_model(model, path):
    model.load_state_dict(torch.load(path))
    print(f"Model loaded: {path}")
    return model


# ----------- 8. 종합 학습 그래프 그리기 함수 ----------- #
def plot_comprehensive_metrics(train_metrics_dict, val_metrics_dict, model_name, filename):
    plt.figure(figsize=(16, 12))

    # 지표 이름과 색상 정의
    metrics = {
        'loss': {'color': 'tab:blue', 'linestyle': '-', 'marker': 'o', 'name': 'Loss'},
        'accuracy': {'color': 'tab:green', 'linestyle': '-', 'marker': 's', 'name': 'Accuracy'},
        'pos_accuracy': {'color': 'tab:red', 'linestyle': '-', 'marker': '^', 'name': 'Pos. Accuracy'},
        'neg_accuracy': {'color': 'tab:purple', 'linestyle': '-', 'marker': 'D', 'name': 'Neg. Accuracy'}
    }

    epochs = range(1, len(train_metrics_dict['loss']) + 1)

    # 두 개의 y축 생성 (왼쪽: 손실, 오른쪽: 정확도)
    fig, ax1 = plt.subplots(figsize=(16, 10))
    ax2 = ax1.twinx()

    # 손실 축 (왼쪽)
    ax1.set_xlabel('Epochs', fontsize=14)
    ax1.set_ylabel('Loss', fontsize=14, color='tab:blue')
    ax1.tick_params(axis='y', labelcolor='tab:blue')
    ax1.grid(True, linestyle='--', alpha=0.3, which='both')

    # 정확도 축 (오른쪽)
    ax2.set_ylabel('Accuracy', fontsize=14, color='tab:green')
    ax2.tick_params(axis='y', labelcolor='tab:green')

    # 선 간격 조정
    line_offset = 0.02

    # 각 지표 플로팅
    lines = []
    for i, (metric, props) in enumerate(metrics.items()):
        if metric == 'loss':
            # 손실 - 왼쪽 축 사용
            ln1, = ax1.plot(epochs, train_metrics_dict[metric],
                            color=props['color'], linestyle=props['linestyle'], marker=props['marker'],
                            markersize=8, linewidth=2, label=f"Train {props['name']}", alpha=0.7)
            ln2, = ax1.plot(epochs, val_metrics_dict[metric],
                            color=props['color'], linestyle='--', marker=props['marker'],
                            markersize=8, linewidth=2, label=f"Val {props['name']}", alpha=1.0)
            lines.extend([ln1, ln2])

            # 최적 손실 포인트 표시
            best_val_epoch = val_metrics_dict[metric].index(min(val_metrics_dict[metric])) + 1
            best_val_value = min(val_metrics_dict[metric])
            ax1.plot(best_val_epoch, best_val_value, 'o', color='tab:blue', markersize=12, fillstyle='none')
            ax1.annotate(f'Best: {best_val_value:.4f}',
                         xy=(best_val_epoch, best_val_value),
                         xytext=(best_val_epoch + 0.3, best_val_value + 0.01),
                         fontsize=12, color='tab:blue')
        else:
            # 정확도 지표 - 오른쪽 축 사용
            offset = (i - 1) * line_offset
            ln1, = ax2.plot(epochs, [x + offset for x in train_metrics_dict[metric]],
                            color=props['color'], linestyle=props['linestyle'], marker=props['marker'],
                            markersize=8, linewidth=2, label=f"Train {props['name']}", alpha=0.7)
            ln2, = ax2.plot(epochs, [x + offset for x in val_metrics_dict[metric]],
                            color=props['color'], linestyle='--', marker=props['marker'],
                            markersize=8, linewidth=2, label=f"Val {props['name']}", alpha=1.0)
            lines.extend([ln1, ln2])

            # 최적 정확도 포인트 표시 (최댓값)
            best_val_epoch = val_metrics_dict[metric].index(max(val_metrics_dict[metric])) + 1
            best_val_value = max(val_metrics_dict[metric])
            ax2.plot(best_val_epoch, best_val_value + offset, 'o', color=props['color'], markersize=12,
                     fillstyle='none')
            ax2.annotate(f'Best: {best_val_value:.4f}',
                         xy=(best_val_epoch, best_val_value + offset),
                         xytext=(best_val_epoch + 0.3, best_val_value + offset + 0.01),
                         fontsize=12, color=props['color'])

    # 축 범위 설정
    loss_min = min(min(train_metrics_dict['loss']), min(val_metrics_dict['loss']))
    loss_max = max(max(train_metrics_dict['loss']), max(val_metrics_dict['loss']))
    loss_margin = (loss_max - loss_min) * 0.1
    ax1.set_ylim(loss_min - loss_margin, loss_max + loss_margin)

    acc_min = min([min(train_metrics_dict[k]) for k in ['accuracy', 'pos_accuracy', 'neg_accuracy']] +
                  [min(val_metrics_dict[k]) for k in ['accuracy', 'pos_accuracy', 'neg_accuracy']])
    acc_max = max([max(train_metrics_dict[k]) for k in ['accuracy', 'pos_accuracy', 'neg_accuracy']] +
                  [max(val_metrics_dict[k]) for k in ['accuracy', 'pos_accuracy', 'neg_accuracy']])
    acc_margin = (acc_max - acc_min) * 0.2
    ax2.set_ylim(acc_min - acc_margin, acc_max + acc_margin + 3 * line_offset)

    # 타이틀 및 레전드 설정
    plt.title(f'{model_name} - Comprehensive Training Metrics', fontsize=16, fontweight='bold')

    # 레전드 통합
    labels = [l.get_label() for l in lines]
    ax1.legend(lines, labels, loc='lower center', bbox_to_anchor=(0.5, -0.15),
               ncol=4, fontsize=12, frameon=True, facecolor='white', edgecolor='gray')

    plt.tight_layout()
    plt.subplots_adjust(bottom=0.15)  # 레전드 공간 확보

    # 폴더 생성 및 저장
    os.makedirs('plots', exist_ok=True)
    plt.savefig(f'plots/{filename}.png', dpi=300, bbox_inches='tight')
    print(f"Comprehensive graph saved: plots/{filename}.png")
    plt.close()


# ----------- 9. 모델별 학습 함수 (GMF, MLP, NeuMF) ----------- #
def train_gmf_model(n_users, n_items, factor_num, device, train_dataloader, val_dataloader, n_epochs):
    print("\n===== Training GMF Model =====")
    model_path = 'models/gmf_model.pth'
    best_model_path = 'models/gmf_model_best.pth'

    # 모델 초기화
    model = GMF(n_users, n_items, factor_num).to(device)
    print(f"GMF model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # 손실 함수 - 가중치 조정 (불균형 보정)
    pos_weight_value = min(train_dataset.pos_weight, 3.0)  # 상한선 설정
    pos_weight = torch.tensor([pos_weight_value]).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    print(f"Loss function: BCEWithLogitsLoss (positive weight: {pos_weight_value:.2f})")

    # 최적화기 설정
    optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=0.00001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

    # 학습 추적을 위한 지표
    train_metrics = {'loss': [], 'accuracy': [], 'pos_accuracy': [], 'neg_accuracy': []}
    val_metrics = {'loss': [], 'accuracy': [], 'pos_accuracy': [], 'neg_accuracy': []}

    best_val_loss = float('inf')
    best_epoch = 0
    no_improve = 0
    patience = 5

    print(
        f"\n{'Epoch':^6} | {'Train Loss':^10} | {'Val Loss':^10} | {'Train Acc':^10} | {'Val Acc':^10} | {'Val Pos Acc':^12} | {'Val Neg Acc':^12}")
    print('-' * 82)

    for epoch in range(1, n_epochs + 1):
        # 훈련
        train_loss = train(model, train_dataloader, optimizer, criterion, device)

        # 검증
        train_results = evaluate(model, train_dataloader, criterion, device)
        val_results = evaluate(model, val_dataloader, criterion, device)

        # 지표 저장
        for metric in ['loss', 'accuracy', 'pos_accuracy', 'neg_accuracy']:
            train_metrics[metric].append(train_results[metric])
            val_metrics[metric].append(val_results[metric])

        # 결과 출력
        print(
            f"\n{'Epoch':^6} | {'Train Loss':^10} | {'Val Loss':^10} | {'Train Acc':^10} | {'Val Acc':^10} | {'Val Pos Acc':^12} | {'Val Neg Acc':^12}")
        print('-' * 82)
        print(
            f"{epoch:6d} | {train_results['loss']:10.6f} | {val_results['loss']:10.6f} | {train_results['accuracy']:10.4f} | {val_results['accuracy']:10.4f} | {val_results['pos_accuracy']:12.4f} | {val_results['neg_accuracy']:12.4f}")

        # 최고 모델 저장
        if val_results['loss'] < best_val_loss:
            best_val_loss = val_results['loss']
            best_epoch = epoch
            no_improve = 0
            save_model(model, best_model_path)
            print(f"  [Improved] New best model saved! (Val Loss: {best_val_loss:.6f})")
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"\nEarly stopping after {patience} epochs without improvement.")
                print(f"Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.6f}")
                break

        # 학습률 스케줄러 업데이트
        scheduler.step(val_results['loss'])

    # 마지막 모델 저장
    save_model(model, model_path)

    print(f"\nGMF model training completed! Best epoch: {best_epoch}, Best val loss: {best_val_loss:.6f}")

    # 종합 그래프 그리기
    plot_comprehensive_metrics(train_metrics, val_metrics, 'GMF Model', 'gmf_comprehensive')

    # 최고 모델 로드
    model = load_model(model, best_model_path)

    return model


def train_mlp_model(n_users, n_items, factor_num, num_layers, dropout, device, train_dataloader, val_dataloader,
                    n_epochs):
    print("\n===== Training MLP Model =====")
    model_path = 'models/mlp_model.pth'
    best_model_path = 'models/mlp_model_best.pth'

    # 모델 초기화
    model = MLP(n_users, n_items, factor_num, num_layers, dropout).to(device)
    print(f"MLP model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # 손실 함수 및 불균형 처리 개선
    pos_weight_value = math.log(1 + train_dataset.pos_weight) * 2
    pos_weight = torch.tensor([pos_weight_value]).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    print(f"Loss function: BCEWithLogitsLoss (positive weight: {pos_weight_value:.2f})")

    # 최적화기 설정
    optimizer = optim.Adam(model.parameters(), lr=0.0003, weight_decay=0.0001)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

    # 학습 추적을 위한 지표
    train_metrics = {'loss': [], 'accuracy': [], 'pos_accuracy': [], 'neg_accuracy': []}
    val_metrics = {'loss': [], 'accuracy': [], 'pos_accuracy': [], 'neg_accuracy': []}

    best_val_loss = float('inf')
    best_epoch = 0
    no_improve = 0
    patience = 5

    print(
        f"\n{'Epoch':^6} | {'Train Loss':^10} | {'Val Loss':^10} | {'Train Acc':^10} | {'Val Acc':^10} | {'Val Pos Acc':^12} | {'Val Neg Acc':^12}")
    print('-' * 82)

    for epoch in range(1, n_epochs + 1):
        # 훈련
        train_loss = train(model, train_dataloader, optimizer, criterion, device)

        # 검증
        train_results = evaluate(model, train_dataloader, criterion, device)
        val_results = evaluate(model, val_dataloader, criterion, device)

        # 지표 저장
        for metric in ['loss', 'accuracy', 'pos_accuracy', 'neg_accuracy']:
            train_metrics[metric].append(train_results[metric])
            val_metrics[metric].append(val_results[metric])

        # 결과 출력
        print(
            f"\n{'Epoch':^6} | {'Train Loss':^10} | {'Val Loss':^10} | {'Train Acc':^10} | {'Val Acc':^10} | {'Val Pos Acc':^12} | {'Val Neg Acc':^12}")
        print('-' * 82)
        print(
            f"{epoch:6d} | {train_results['loss']:10.6f} | {val_results['loss']:10.6f} | {train_results['accuracy']:10.4f} | {val_results['accuracy']:10.4f} | {val_results['pos_accuracy']:12.4f} | {val_results['neg_accuracy']:12.4f}")

        # 최고 모델 저장
        if val_results['loss'] < best_val_loss:
            best_val_loss = val_results['loss']
            best_epoch = epoch
            no_improve = 0
            save_model(model, best_model_path)
            print(f"  [Improved] New best model saved! (Val Loss: {best_val_loss:.6f})")
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"\nEarly stopping after {patience} epochs without improvement.")
                print(f"Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.6f}")
                break

        # 학습률 스케줄러 업데이트
        scheduler.step(val_results['loss'])

    # 마지막 모델 저장
    save_model(model, model_path)

    print(f"\nMLP model training completed! Best epoch: {best_epoch}, Best val loss: {best_val_loss:.6f}")

    # 종합 그래프 그리기
    plot_comprehensive_metrics(train_metrics, val_metrics, 'MLP Model', 'mlp_comprehensive')

    # 최고 모델 로드
    model = load_model(model, best_model_path)

    return model


def train_neumf_model(n_users, n_items, factor_num, num_layers, dropout, alpha, device,
                      train_dataloader, val_dataloader, gmf_model, mlp_model, n_epochs):
    print("\n===== Training NeuMF Model (with pre-trained weights) =====")
    model_path = 'models/neumf_model.pth'
    best_model_path = 'models/neumf_model_best.pth'

    # 모델 초기화
    model = NeuMF(n_users, n_items, factor_num, num_layers, dropout, alpha).to(device)
    print(f"NeuMF model parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}")

    # 사전 학습된 가중치 로드
    model.load_pretrained(gmf_model, mlp_model)
    print("Pre-trained weights loaded successfully")

    # 손실 함수
    pos_weight_value = train_dataset.pos_weight
    pos_weight = torch.tensor([pos_weight_value]).to(device)
    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)
    print(f"Loss function: BCEWithLogitsLoss (positive weight: {pos_weight_value:.2f})")

    # 최적화기 설정
    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)

    # 학습 추적을 위한 지표
    train_metrics = {'loss': [], 'accuracy': [], 'pos_accuracy': [], 'neg_accuracy': []}
    val_metrics = {'loss': [], 'accuracy': [], 'pos_accuracy': [], 'neg_accuracy': []}

    best_val_loss = float('inf')
    best_epoch = 0
    no_improve = 0
    patience = 5

    print(
        f"\n{'Epoch':^6} | {'Train Loss':^10} | {'Val Loss':^10} | {'Train Acc':^10} | {'Val Acc':^10} | {'Val Pos Acc':^12} | {'Val Neg Acc':^12}")
    print('-' * 82)

    for epoch in range(1, n_epochs + 1):
        # 훈련
        train_loss = train(model, train_dataloader, optimizer, criterion, device)

        # 검증
        train_results = evaluate(model, train_dataloader, criterion, device)
        val_results = evaluate(model, val_dataloader, criterion, device)

        # 지표 저장
        for metric in ['loss', 'accuracy', 'pos_accuracy', 'neg_accuracy']:
            train_metrics[metric].append(train_results[metric])
            val_metrics[metric].append(val_results[metric])

        # 결과 출력
        print(
            f"\n{'Epoch':^6} | {'Train Loss':^10} | {'Val Loss':^10} | {'Train Acc':^10} | {'Val Acc':^10} | {'Val Pos Acc':^12} | {'Val Neg Acc':^12}")
        print('-' * 82)
        print(
            f"{epoch:6d} | {train_results['loss']:10.6f} | {val_results['loss']:10.6f} | {train_results['accuracy']:10.4f} | {val_results['accuracy']:10.4f} | {val_results['pos_accuracy']:12.4f} | {val_results['neg_accuracy']:12.4f}")

        # 최고 모델 저장
        if val_results['loss'] < best_val_loss:
            best_val_loss = val_results['loss']
            best_epoch = epoch
            no_improve = 0
            save_model(model, best_model_path)
            print(f"  [Improved] New best model saved! (Val Loss: {best_val_loss:.6f})")
        else:
            no_improve += 1
            if no_improve >= patience:
                print(f"\nEarly stopping after {patience} epochs without improvement.")
                print(f"Best epoch: {best_epoch}, Best validation loss: {best_val_loss:.6f}")
                break

        # 학습률 스케줄러 업데이트
        scheduler.step(val_results['loss'])

    # 마지막 모델 저장
    save_model(model, model_path)

    print(f"\nNeuMF model training completed! Best epoch: {best_epoch}, Best val loss: {best_val_loss:.6f}")

    # 종합 그래프 그리기
    plot_comprehensive_metrics(train_metrics, val_metrics, 'NeuMF Model', 'neumf_comprehensive')

    # 최고 모델 로드
    model = load_model(model, best_model_path)

    return model


# ----------- 10. 메인 실행 코드 ----------- #
if __name__ == "__main__":
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"사용 장치: {device}")

    # 하이퍼파라미터 설정
    factor_num = 64
    num_layers = 4
    dropout = 0.6
    n_epochs = 20
    alpha = 0.5  # GMF와 MLP의 균형을 위한 하이퍼파라미터

    # 1. GMF 모델 학습
    gmf_model = train_gmf_model(n_users, n_items, factor_num, device, train_dataloader, val_dataloader, n_epochs)

    # 2. MLP 모델 학습
    mlp_model = train_mlp_model(n_users, n_items, factor_num, num_layers, dropout, device, train_dataloader,
                                val_dataloader, n_epochs)

    # 3. NeuMF 모델 학습 (사전 학습된 가중치 사용)
    neumf_model = train_neumf_model(n_users, n_items, factor_num, num_layers, dropout, alpha, device,
                                    train_dataloader, val_dataloader, gmf_model, mlp_model, n_epochs)

    # 4. 테스트 데이터로 최종 평가
    print("\n===== 최종 테스트 평가 =====")
    test_criterion = nn.BCEWithLogitsLoss()  # 평가에는 일반적인 BCE 사용

    print("\nGMF 모델 테스트:")
    gmf_test_metrics = evaluate(gmf_model, test_dataloader, test_criterion, device)
    print(f"  테스트 손실: {gmf_test_metrics['loss']:.6f}, 정확도: {gmf_test_metrics['accuracy']:.4f}")
    print(f"  긍정 정확도: {gmf_test_metrics['pos_accuracy']:.4f}, 부정 정확도: {gmf_test_metrics['neg_accuracy']:.4f}")

    print("\nMLP 모델 테스트:")
    mlp_test_metrics = evaluate(mlp_model, test_dataloader, test_criterion, device)
    print(f"  테스트 손실: {mlp_test_metrics['loss']:.6f}, 정확도: {mlp_test_metrics['accuracy']:.4f}")
    print(f"  긍정 정확도: {mlp_test_metrics['pos_accuracy']:.4f}, 부정 정확도: {mlp_test_metrics['neg_accuracy']:.4f}")

    print("\nNeuMF 모델 테스트:")
    neumf_test_metrics = evaluate(neumf_model, test_dataloader, test_criterion, device)
    print(f"  테스트 손실: {neumf_test_metrics['loss']:.6f}, 정확도: {neumf_test_metrics['accuracy']:.4f}")
    print(f"  긍정 정확도: {neumf_test_metrics['pos_accuracy']:.4f}, 부정 정확도: {neumf_test_metrics['neg_accuracy']:.4f}")

    print("\n모든 학습 및 평가 완료!")
