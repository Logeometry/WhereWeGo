import numpy as np
import json
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import os
import time
import copy
import random
from tqdm import tqdm
from datetime import datetime
import matplotlib.pyplot as plt
"""
ì´ ë²„ì „ì€ ë…¼ë¬¸ê³¼ ë” ìœ ì‚¬í•œ ì„¤ì •ì„ ë”°ë¥´ëŠ” BPR êµ¬í˜„ì„.

ë³€ê²½ ì‚¬í•­:
- ìµœì í™” í•¨ìˆ˜: Adam â†’ S.G.D
- í•™ìŠµë¥ : 0.01 â†’ 0.05
- ì„ë² ë”© ì´ˆê¸°í™”: 0.001 â†’ 0.1
- í•™ìŠµë¥  ê°ì†Œ ìŠ¤ì¼€ì¤„ëŸ¬ â†’ L2 ì •ê·œí™” ë„ì…
    user_reg: 0.0025
    item_reg: 0.00025

ì •ê·œí™” ê³„ìˆ˜ëŠ” ì‚¬ìš©ì ì„ë² ë”©ì— ë” í¬ê²Œ ì ìš©ë˜ì—ˆìŒ
ì´ëŠ” ê°œì¸í™” ì¶”ì²œì—ì„œ ìœ ì € ë²¡í„°ê°€ ê°ê¸° ë‹¤ë¥¸ ì‚¬ìš©ì íŠ¹ì„±ì„ í•™ìŠµí•˜ê¸° ë•Œë¬¸
ê³¼ì í•©ì„ ë°©ì§€í•˜ê¸° ìœ„í•œ ì¡°ì¹˜ë¡œ ì‚¬ìš©
ë°˜ë©´, ì•„ì´í…œ ë²¡í„°ëŠ” ì—¬ëŸ¬ ì‚¬ìš©ìê°€ ê³µìœ í•˜ëŠ” ì •ë³´ì„
ìƒëŒ€ì ìœ¼ë¡œ ì¼ë°˜í™”ê°€ ì‰¬ì›Œ ë” ì•½í•œ ì •ê·œí™”ë¥¼ ì ìš©í•¨.
"""
class BPRDataset(Dataset):
    def __init__(self, triplet_file):
        """
        íŠ¸ë¦¬í”Œë › íŒŒì¼ë¡œë¶€í„° BPR ë°ì´í„°ì…‹ ìƒì„±
        
        Args:
            triplet_file: íŠ¸ë¦¬í”Œë › JSON íŒŒì¼ ê²½ë¡œ
        """
        start_time = time.time()
        
        print(f"ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘: {triplet_file}")
        with open(triplet_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        self.triplets = data['triplets']
        self.metadata = data['metadata']
        self.user_id_map = data.get('user_id_map', {})
        self.business_id_map = data.get('business_id_map', {})
        
        # ë°ì´í„°ì…‹ ì •ë³´ ì¶œë ¥
        print(f"ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ:")
        print(f"- ì´ íŠ¸ë¦¬í”Œë › ìˆ˜: {len(self.triplets)}")
        print(f"- ì‚¬ìš©ì ìˆ˜: {self.metadata.get('num_users', 0)}")
        print(f"- ë¹„ì¦ˆë‹ˆìŠ¤ ìˆ˜: {self.metadata.get('num_businesses', 0)}")
        
        # NumPy ë°°ì—´ë¡œ ë³€í™˜
        if isinstance(self.triplets[0], list):
            self.data = np.array(self.triplets, dtype=np.int64)
        else:
            # tripletsê°€ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì¸ ê²½ìš°
            self.data = np.array([[t.get('user', 0), t.get('item', 0), t.get('rating', 0)] 
                               for t in self.triplets], dtype=np.int64)
        
        # Positiveì™€ Negative ìƒ˜í”Œ ë¶„ë¦¬
        self.pos_samples = self.data[self.data[:, 2] == 1]
        self.neg_samples = self.data[self.data[:, 2] == 0]
        
        # ì¶”ê°€: ì‚¬ìš©ìë³„ ìƒí˜¸ì‘ìš© ì •ë³´ êµ¬ì„± (LearnBPR ì•Œê³ ë¦¬ì¦˜ ìœ„í•¨)
        self.user_pos_items = {}
        self.user_neg_items = {}
        
        # ì‚¬ìš©ìë³„ ê¸ì •/ë¶€ì • ìƒí˜¸ì‘ìš© í•­ëª© ì •ë¦¬
        for user_id in np.unique(self.pos_samples[:, 0]):
            user_id = int(user_id)
            # í˜„ì¬ ì‚¬ìš©ìì˜ ëª¨ë“  positive ìƒí˜¸ì‘ìš©
            pos_items = self.pos_samples[self.pos_samples[:, 0] == user_id][:, 1]
            self.user_pos_items[user_id] = pos_items
            
            # í˜„ì¬ ì‚¬ìš©ìì˜ ëª¨ë“  negative ìƒí˜¸ì‘ìš©
            neg_items = self.neg_samples[self.neg_samples[:, 0] == user_id][:, 1]
            if len(neg_items) > 0:
                self.user_neg_items[user_id] = neg_items
            else:
                # ëª…ì‹œì ì¸ negative ìƒí˜¸ì‘ìš©ì´ ì—†ëŠ” ê²½ìš° ëª¨ë“  non-positive í•­ëª©ì„ negativeë¡œ ê°„ì£¼
                num_items = self.metadata.get('num_businesses', 0)
                self.user_neg_items[user_id] = np.setdiff1d(np.arange(num_items), pos_items)
        
        print(f"- Positive ìƒ˜í”Œ: {len(self.pos_samples)}")
        print(f"- Negative ìƒ˜í”Œ: {len(self.neg_samples)}")
        print(f"- ì‚¬ìš©ì ìˆ˜: {len(self.user_pos_items)}")
        print(f"- ë°ì´í„° ë¡œë”© ì‹œê°„: {time.time() - start_time:.2f}ì´ˆ")
    
    def get_user_data(self):
        """
        LearnBPR ì•Œê³ ë¦¬ì¦˜ì„ ìœ„í•œ ì‚¬ìš©ì ë°ì´í„° ë°˜í™˜
        """
        return {
            'users': list(self.user_pos_items.keys()),
            'user_pos_items': self.user_pos_items,
            'user_neg_items': self.user_neg_items
        }
    
    def __len__(self):
        return len(self.pos_samples)
    
    def __getitem__(self, idx):
        # Positive ìƒ˜í”Œ ê°€ì ¸ì˜¤ê¸°
        user_id, pos_item, _ = self.pos_samples[idx]
        user_id = int(user_id)
        
        # Negative ìƒ˜í”Œ ì„ íƒ
        if user_id in self.user_neg_items and len(self.user_neg_items[user_id]) > 0:
            neg_items = self.user_neg_items[user_id]
            neg_item = np.random.choice(neg_items)
        else:
            # ê·¹ë‹¨ì ì¸ ê²½ìš°: ëª¨ë“  ì•„ì´í…œì´ positiveì¸ ê²½ìš°
            num_items = self.metadata.get('num_businesses', 10000)
            neg_item = np.random.randint(0, num_items)
        
        return {
            'user': torch.tensor([user_id], dtype=torch.long),
            'pos_item': torch.tensor([pos_item], dtype=torch.long),
            'neg_item': torch.tensor([neg_item], dtype=torch.long)
        }


# 2. BPR ëª¨ë¸ ì •ì˜
class BPRModel(nn.Module):
    def __init__(self, num_users, num_items, embedding_dim=128):
        """
        Bayesian Personalized Ranking ëª¨ë¸
        
        Args:
            num_users: ì‚¬ìš©ì ìˆ˜
            num_items: ì•„ì´í…œ(ë¹„ì¦ˆë‹ˆìŠ¤) ìˆ˜
            embedding_dim: ì„ë² ë”© ì°¨ì›
        """
        super(BPRModel, self).__init__()
        
        self.embedding_dim = embedding_dim
        print(f"BPRModel ì´ˆê¸°í™”: ì„ë² ë”© ì°¨ì› = {embedding_dim}")
        
        # ì‚¬ìš©ì ì„ë² ë”©
        self.user_embeddings = nn.Embedding(num_users, embedding_dim)
        # ì•„ì´í…œ ì„ë² ë”©
        self.item_embeddings = nn.Embedding(num_items, embedding_dim)
        
        # ì„ë² ë”© ì´ˆê¸°í™” (ë…¼ë¬¸ì˜ í‰ê·  0, í‘œì¤€í¸ì°¨ 0.1 ì„¤ì •)
        nn.init.normal_(self.user_embeddings.weight, std=0.1)
        nn.init.normal_(self.item_embeddings.weight, std=0.1)
        
        # ì°¨ì› ì €ì¥
        self.num_users = num_users
        self.num_items = num_items
    
    def forward(self, user, pos_item, neg_item):
        """
        BPR ëª¨ë¸ í¬ì›Œë“œ íŒ¨ìŠ¤
        
        Args:
            user: ì‚¬ìš©ì ì¸ë±ìŠ¤
            pos_item: ê¸ì •ì  ì•„ì´í…œ ì¸ë±ìŠ¤
            neg_item: ë¶€ì •ì  ì•„ì´í…œ ì¸ë±ìŠ¤
            
        Returns:
            x_uij: ê¸ì •ì  ì•„ì´í…œê³¼ ë¶€ì •ì  ì•„ì´í…œ ê°„ì˜ ì ìˆ˜ ì°¨ì´
            user_embedding: ì‚¬ìš©ì ì„ë² ë”©
            pos_item_embedding: ê¸ì •ì  ì•„ì´í…œ ì„ë² ë”©
            neg_item_embedding: ë¶€ì •ì  ì•„ì´í…œ ì„ë² ë”©
        """
        # ì„ë² ë”© ê²€ìƒ‰
        user_embedding = self.user_embeddings(user)
        pos_item_embedding = self.item_embeddings(pos_item)
        neg_item_embedding = self.item_embeddings(neg_item)
        
        # ì ìˆ˜ ê³„ì‚°
        pos_scores = torch.sum(user_embedding * pos_item_embedding, dim=-1)
        neg_scores = torch.sum(user_embedding * neg_item_embedding, dim=-1)
        
        # ë…¼ë¬¸ì—ì„œì˜ x_uij ëª…ì‹œì  ê³„ì‚°
        x_uij = pos_scores - neg_scores
        
        return x_uij, user_embedding, pos_item_embedding, neg_item_embedding
    
    def predict(self, user_indices, item_indices=None):
        """
        ì‚¬ìš©ì-ì•„ì´í…œ ì ìˆ˜ ì˜ˆì¸¡
        """
        device = self.user_embeddings.weight.device
        
        # ë‹¨ì¼ ì‚¬ìš©ì, ëª¨ë“  ì•„ì´í…œ ì ìˆ˜ ê³„ì‚°
        if item_indices is None:
            users = torch.tensor([user_indices], dtype=torch.long, device=device)
            user_embedding = self.user_embeddings(users)
            all_item_embeddings = self.item_embeddings.weight
            
            scores = torch.matmul(user_embedding, all_item_embeddings.t())
            return scores.squeeze()
        
        # ì§€ì •ëœ ì‚¬ìš©ì-ì•„ì´í…œ í˜ì–´ ì ìˆ˜ ê³„ì‚°
        users = torch.tensor(user_indices, dtype=torch.long, device=device)
        items = torch.tensor(item_indices, dtype=torch.long, device=device)
        
        user_embedding = self.user_embeddings(users)
        item_embedding = self.item_embeddings(items)
        
        return torch.sum(user_embedding * item_embedding, dim=-1)


# 3. BPR ì†ì‹¤ í•¨ìˆ˜ (ë…¼ë¬¸ì— ì¶©ì‹¤í•œ ë²„ì „)
class BPRLoss(nn.Module):
    def __init__(self):
        super(BPRLoss, self).__init__()
    
    def forward(self, x_uij, user_emb=None, pos_item_emb=None, neg_item_emb=None, lambda_u=0.0025, lambda_i=0.00025):
        """
        BPR ì†ì‹¤ ê³„ì‚°: -ln(sigmoid(x_uij)) + Î»_u * ||u||Â² + Î»_i * (||i||Â² + ||j||Â²)
        
        Args:
            x_uij: ì‚¬ìš©ì uê°€ ì•„ì´í…œ ië¥¼ ì•„ì´í…œ jë³´ë‹¤ ì„ í˜¸í•˜ëŠ” ì •ë„
            user_emb: ì‚¬ìš©ì ì„ë² ë”©
            pos_item_emb: ê¸ì •ì  ì•„ì´í…œ ì„ë² ë”©
            neg_item_emb: ë¶€ì •ì  ì•„ì´í…œ ì„ë² ë”©
            lambda_u: ì‚¬ìš©ì ì„ë² ë”©ì— ëŒ€í•œ ì •ê·œí™” ê³„ìˆ˜
            lambda_i: ì•„ì´í…œ ì„ë² ë”©ì— ëŒ€í•œ ì •ê·œí™” ê³„ìˆ˜
            
        Returns:
            ì†ì‹¤ê°’
        """
        # ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ ìœ„í•œ epsilon
        epsilon = 1e-8
        
        # ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ì†ì‹¤ í•¨ìˆ˜ (ì‹ 8)
        bpr_loss = -torch.log(torch.sigmoid(x_uij) + epsilon)
        
        # ë¶„ë¦¬ëœ ì •ê·œí™” ì ìš© (ì‹ 9~11)
        if user_emb is not None and pos_item_emb is not None and neg_item_emb is not None:
            user_reg = lambda_u * torch.norm(user_emb, p=2, dim=1)
            pos_item_reg = lambda_i * torch.norm(pos_item_emb, p=2, dim=1)
            neg_item_reg = lambda_i * torch.norm(neg_item_emb, p=2, dim=1)
            
            # ì´ ì†ì‹¤ = BPR ì†ì‹¤ + ì‚¬ìš©ì ì •ê·œí™” + ì•„ì´í…œ ì •ê·œí™”
            total_loss = bpr_loss + user_reg + pos_item_reg + neg_item_reg
            return total_loss.mean()
        
        return bpr_loss.mean()


# 5. í‰ê°€ í•¨ìˆ˜
def evaluate_model(model, dataset, top_k=10, num_users=100, device="cuda" if torch.cuda.is_available() else "cpu", verbose=True):
    """
    ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
    
    Args:
        model: í‰ê°€í•  BPR ëª¨ë¸
        dataset: ë°ì´í„°ì…‹
        top_k: ì¶”ì²œ ì•„ì´í…œ ìˆ˜
        num_users: í‰ê°€í•  ì‚¬ìš©ì ìˆ˜
        device: ê³„ì‚° ì¥ì¹˜
        verbose: ìƒì„¸ ì¶œë ¥ ì—¬ë¶€
    
    Returns:
        í‰ê°€ ì§€í‘œ ë”•ì…”ë„ˆë¦¬
    """
    model.eval()
    model = model.to(device)
    
    # í‰ê°€í•  ì‚¬ìš©ì ì„ íƒ
    user_data = dataset.get_user_data()
    users = user_data['users']
    user_pos_items = user_data['user_pos_items']
    
    if num_users is not None and num_users < len(users):
        eval_users = np.random.choice(users, num_users, replace=False)
    else:
        eval_users = users
    
    # í‰ê°€ ì§€í‘œ
    metrics = {
        f'MAP@{top_k}': [],
        f'Recall@{top_k}': [],
        f'NDCG@{top_k}': []
    }
    
    if verbose:
        print(f"\ní‰ê°€ ì¤‘ ({len(eval_users)} ì‚¬ìš©ì)...")
        progress_bar = tqdm(eval_users)
    else:
        progress_bar = eval_users
    
    with torch.no_grad():
        for u in progress_bar:
            # í•™ìŠµ ì•„ì´í…œ (ë§ˆìŠ¤í‚¹ìš©)
            train_items = user_pos_items[u]
            
            # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ìƒì„± (í•™ìŠµ ì•„ì´í…œì˜ 20%ë¥¼ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„ë¦¬)
            np.random.shuffle(train_items)
            split_idx = int(len(train_items) * 0.8)
            test_items = train_items[split_idx:]
            actual_train_items = train_items[:split_idx]
            
            if len(test_items) == 0:
                continue
                
            # ì‚¬ìš©ìì˜ ëª¨ë“  ì•„ì´í…œ ì ìˆ˜ ì˜ˆì¸¡
            scores = model.predict(u)
            scores = scores.cpu().numpy()
            
            # í•™ìŠµ ì•„ì´í…œ ë§ˆìŠ¤í‚¹
            scores[actual_train_items] = -np.inf
            
            # Top-K ì¶”ì²œ ì•„ì´í…œ
            recommended_items = np.argsort(-scores)[:top_k]
            
            # í‰ê°€ ì§€í‘œ ê³„ì‚°
            # MAP@K
            precision = len(set(recommended_items) & set(test_items)) / top_k
            metrics[f'MAP@{top_k}'].append(precision)
            
            # Recall@K
            recall = len(set(recommended_items) & set(test_items)) / len(test_items)
            metrics[f'Recall@{top_k}'].append(recall)
            
            # NDCG@K ê³„ì‚°
            dcg = 0
            for i, item in enumerate(recommended_items):
                if item in test_items:
                    dcg += 1 / np.log2(i + 2)
            
            idcg = 0
            for i in range(min(len(test_items), top_k)):
                idcg += 1 / np.log2(i + 2)
            
            ndcg = dcg / idcg if idcg > 0 else 0
            metrics[f'NDCG@{top_k}'].append(ndcg)
    
    # í‰ê·  ì§€í‘œ ê³„ì‚°
    avg_metrics = {}
    for metric, values in metrics.items():
        if values:
            avg_metrics[metric] = np.mean(values)
        else:
            avg_metrics[metric] = 0.0
    
    # ê²°ê³¼ ì¶œë ¥
    if verbose:
        print("\ní‰ê°€ ê²°ê³¼:")
        for metric, value in avg_metrics.items():
            print(f"- {metric}: {value:.4f}")
    
    return avg_metrics


# ì„±ëŠ¥ ì§€í‘œ ì‹œê°í™” í•¨ìˆ˜ ì¶”ê°€
def plot_metrics(epochs, metrics_history, save_path="./bpr_model/performance_metrics.png"):
    """
    í•™ìŠµ ê³¼ì •ì˜ ì„±ëŠ¥ ì§€í‘œë¥¼ ì‹œê°í™”
    
    Args:
        epochs: ì—í¬í¬ ë¦¬ìŠ¤íŠ¸
        metrics_history: ì—í¬í¬ë³„ ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡
        save_path: ê·¸ë˜í”„ ì €ì¥ ê²½ë¡œ
    """
    plt.figure(figsize=(15, 6))
    
    # ì§€í‘œë³„ ìƒ‰ìƒ ì •ì˜
    colors = {
        'MAP': 'blue',
        'Recall': 'red',
        'NDCG': 'green'
    }
    
    # ê° ì§€í‘œë³„ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    for metric_name in ['MAP', 'Recall', 'NDCG']:
        metric_values = [metrics[f"{metric_name}@10"] for metrics in metrics_history]
        plt.plot(epochs, metric_values, f"-o", color=colors[metric_name], label=f"{metric_name}@10")
    
    plt.xlabel('Epoch')
    plt.ylabel('Metric Value')
    plt.title('BPR Model Performance Metrics')
    plt.legend()
    plt.grid(True)
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ í™•ì¸
    save_dir = os.path.dirname(save_path)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    # ê·¸ë˜í”„ ì €ì¥
    plt.savefig(save_path)
    print(f"ì„±ëŠ¥ ì§€í‘œ ê·¸ë˜í”„ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    return plt


# 4. LearnBPR ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (ë…¼ë¬¸ì— ì¶©ì‹¤í•œ ë°©ì‹)
def learn_bpr_algorithm(
    dataset, 
    model,
    num_epochs=50,
    learning_rate=0.05,
    lambda_user=0.0025,
    lambda_item=0.00025,
    num_samples_per_epoch=50000,
    patience=5,
    device="cuda" if torch.cuda.is_available() else "cpu",
    save_path="./bpr_model/original_bpr_model.pt",
    eval_interval=5,  
    eval_users=100    
):
    """
    BPR ë…¼ë¬¸ì˜ LearnBPR ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ (Algorithm 1)
    
    Args:
        dataset: BPR ë°ì´í„°ì…‹
        model: BPR ëª¨ë¸
        num_epochs: í•™ìŠµ ì—í¬í¬ ìˆ˜
        learning_rate: í•™ìŠµë¥ 
        lambda_user: ì‚¬ìš©ì ì„ë² ë”©ì— ëŒ€í•œ ì •ê·œí™” ê³„ìˆ˜
        lambda_item: ì•„ì´í…œ ì„ë² ë”©ì— ëŒ€í•œ ì •ê·œí™” ê³„ìˆ˜
        num_samples_per_epoch: ê° ì—í¬í¬ë‹¹ ìƒ˜í”Œë§í•  íŠ¸ë¦¬í”Œë › ìˆ˜
        patience: ê²€ì¦ ì†ì‹¤ í–¥ìƒ ì—†ì„ ë•Œ ì¡°ê¸° ì¢…ë£Œ ì—í¬í¬ ìˆ˜
        device: í•™ìŠµ ì¥ì¹˜
        save_path: ëª¨ë¸ ì €ì¥ ê²½ë¡œ
        eval_interval: í‰ê°€ ê°„ê²© (ì—í¬í¬)
        eval_users: í‰ê°€í•  ì‚¬ìš©ì ìˆ˜
    
    Returns:
        í•™ìŠµëœ ëª¨ë¸, ì†ì‹¤ ë¦¬ìŠ¤íŠ¸, ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡
    """
    print(f"\nLearnBPR ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµ ì‹œì‘ (ë…¼ë¬¸ ì›ë³¸ êµ¬í˜„)")
    print(f"- í•™ìŠµë¥ : {learning_rate}")
    print(f"- ì‚¬ìš©ì ì •ê·œí™”(Î»_u): {lambda_user}")
    print(f"- ì•„ì´í…œ ì •ê·œí™”(Î»_i): {lambda_item}")
    print(f"- ì—í¬í¬ë‹¹ ìƒ˜í”Œ ìˆ˜: {num_samples_per_epoch}")
    print(f"- í‰ê°€ ê°„ê²©: {eval_interval} ì—í¬í¬")
    
    model = model.to(device)
    model.train()
    
    # ì†ì‹¤ í•¨ìˆ˜
    bpr_loss_fn = BPRLoss()
    
    # ë°ì´í„° ì¤€ë¹„
    user_data = dataset.get_user_data()
    users = user_data['users']
    user_pos_items = user_data['user_pos_items']
    user_neg_items = user_data['user_neg_items']
    
    # í•™ìŠµ í†µê³„
    losses = []
    eval_epochs = []
    metrics_history = []
    best_loss = float('inf')
    wait = 0
    best_model = None
    
    # í•™ìŠµ ë£¨í”„
    for epoch in range(num_epochs):
        start_time = time.time()
        epoch_loss = 0.0
        
        # ê° ì—í¬í¬ë§ˆë‹¤ ì§€ì •ëœ ìˆ˜ì˜ (u,i,j) íŠ¸ë¦¬í”Œë › ìƒ˜í”Œë§
        with tqdm(total=num_samples_per_epoch, desc=f"Epoch {epoch+1}/{num_epochs}") as pbar:
            for sample_idx in range(num_samples_per_epoch):
                # 1. ë¬´ì‘ìœ„ë¡œ ì‚¬ìš©ì ì„ íƒ
                u = random.choice(users)
                
                # ì‚¬ìš©ìê°€ ê¸ì •ì /ë¶€ì •ì  ìƒí˜¸ì‘ìš©ì´ ëª¨ë‘ ìˆëŠ”ì§€ í™•ì¸
                if len(user_pos_items[u]) == 0 or len(user_neg_items[u]) == 0:
                    continue
                
                # 2. ì„ íƒí•œ ì‚¬ìš©ìì˜ ê¸ì •ì  ì•„ì´í…œ ë¬´ì‘ìœ„ ì„ íƒ
                i = np.random.choice(user_pos_items[u])
                
                # 3. ì„ íƒí•œ ì‚¬ìš©ìì˜ ë¶€ì •ì  ì•„ì´í…œ ë¬´ì‘ìœ„ ì„ íƒ
                j = np.random.choice(user_neg_items[u])
                
                # í…ì„œë¡œ ë³€í™˜ ë° ì¥ì¹˜ ì´ë™
                u_tensor = torch.tensor([u], dtype=torch.long, device=device)
                i_tensor = torch.tensor([i], dtype=torch.long, device=device)
                j_tensor = torch.tensor([j], dtype=torch.long, device=device)
                
                # 4. ëª¨ë¸ í¬ì›Œë“œ íŒ¨ìŠ¤ë¡œ x_uij ê³„ì‚°
                x_uij, user_emb, pos_item_emb, neg_item_emb = model(u_tensor, i_tensor, j_tensor)
                
                # 5. BPR ì†ì‹¤ ê³„ì‚° (ë¶„ë¦¬ëœ ì •ê·œí™” ì ìš©)
                loss = bpr_loss_fn(
                    x_uij,
                    user_emb=user_emb,
                    pos_item_emb=pos_item_emb,
                    neg_item_emb=neg_item_emb,
                    lambda_u=lambda_user,
                    lambda_i=lambda_item
                )
                
                # 6. ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
                # ì—¬ê¸°ì„œëŠ” ì§ì ‘ ê·¸ë˜ë””ì–¸íŠ¸ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ì‹ ëŒ€ì‹  PyTorchì˜ ìë™ ë¯¸ë¶„ ì‚¬ìš©
                
                # 7-9. ë§¤ê°œë³€ìˆ˜ ì—…ë°ì´íŠ¸ (SGD ë°©ì‹)
                # ì‚¬ìš©ì ì„ë² ë”© ì—…ë°ì´íŠ¸
                u_grad = torch.autograd.grad(
                    loss, user_emb, retain_graph=True
                )[0]
                model.user_embeddings.weight.data[u] -= learning_rate * u_grad.squeeze()
                
                # ê¸ì •ì  ì•„ì´í…œ ì„ë² ë”© ì—…ë°ì´íŠ¸
                i_grad = torch.autograd.grad(
                    loss, pos_item_emb, retain_graph=True
                )[0]
                model.item_embeddings.weight.data[i] -= learning_rate * i_grad.squeeze()
                
                # ë¶€ì •ì  ì•„ì´í…œ ì„ë² ë”© ì—…ë°ì´íŠ¸
                j_grad = torch.autograd.grad(
                    loss, neg_item_emb
                )[0]
                model.item_embeddings.weight.data[j] -= learning_rate * j_grad.squeeze()
                
                epoch_loss += loss.item()
                pbar.update(1)
                
                # ì§„í–‰ ìƒí™© ì—…ë°ì´íŠ¸
                if sample_idx % 1000 == 0:
                    pbar.set_postfix({'loss': epoch_loss / (sample_idx + 1)})
        
        # ì—í¬í¬ í‰ê·  ì†ì‹¤
        avg_epoch_loss = epoch_loss / num_samples_per_epoch
        losses.append(avg_epoch_loss)
        
        epoch_time = time.time() - start_time
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {avg_epoch_loss:.6f} - Time: {epoch_time:.2f}s")
        
        # ì£¼ê¸°ì ìœ¼ë¡œ ì„±ëŠ¥ í‰ê°€
        if (epoch + 1) % eval_interval == 0 or epoch == num_epochs - 1:
            print(f"\nì—í¬í¬ {epoch+1} ì„±ëŠ¥ í‰ê°€:")
            eval_metrics = evaluate_model(
                model=model,
                dataset=dataset,
                top_k=10,
                num_users=eval_users,
                device=device,
                verbose=True
            )
            
            # ì„±ëŠ¥ ì§€í‘œ ê¸°ë¡
            eval_epochs.append(epoch + 1)
            metrics_history.append(eval_metrics)
        
        # ìµœê³  ëª¨ë¸ ì €ì¥ ë° ì¡°ê¸° ì¢…ë£Œ í™•ì¸
        if avg_epoch_loss < best_loss:
            best_loss = avg_epoch_loss
            best_model = copy.deepcopy(model.state_dict())
            wait = 0
            print(f"  ğŸ‘ New best model (Loss: {best_loss:.6f})")
        else:
            wait += 1
            if wait >= patience:
                print(f"Early stopping after {epoch+1} epochs (no improvement for {patience} epochs)")
                break
    
    # ìµœê³  ëª¨ë¸ ë³µì›
    if best_model is not None:
        model.load_state_dict(best_model)
    
    # ì†ì‹¤ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    plt.figure(figsize=(10, 5))
    plt.plot(range(1, len(losses)+1), losses)
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.title('LearnBPR Training Loss')
    plt.grid(True)
    
    # ì €ì¥ ë””ë ‰í† ë¦¬ í™•ì¸
    save_dir = os.path.dirname(save_path)
    if not os.path.exists(save_dir):
        os.makedirs(save_dir)
    
    # ê·¸ë˜í”„ ì €ì¥
    plt.savefig(os.path.join(save_dir, 'learnbpr_loss.png'))
    
    # ì„±ëŠ¥ ì§€í‘œ ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
    if metrics_history:
        metrics_plot = plot_metrics(
            eval_epochs, 
            metrics_history, 
            save_path=os.path.join(save_dir, 'learnbpr_metrics.png')
        )
    
    # ëª¨ë¸ ì €ì¥
    torch.save({
        'model_state': model.state_dict(),
        'losses': losses,
        'metrics_history': metrics_history,
        'eval_epochs': eval_epochs,
        'best_epoch': len(losses) - wait,
        'params': {
            'learning_rate': learning_rate,
            'lambda_user': lambda_user,
            'lambda_item': lambda_item,
            'num_samples_per_epoch': num_samples_per_epoch,
            'final_loss': losses[-1]
        }
    }, save_path)
    
    print(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")
    
    return model, losses, metrics_history


# 6. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main(
    triplet_file="philly_bpr_triplets.json",
    embedding_dim=128,
    learning_rate=0.05,  # ë…¼ë¬¸ì—ì„œëŠ” 0.05ë¥¼ ê¶Œì¥
    lambda_user=0.0025,  # ì‚¬ìš©ì ì„ë² ë”© ì •ê·œí™” ê³„ìˆ˜
    lambda_item=0.00025, # ì•„ì´í…œ ì„ë² ë”© ì •ê·œí™” ê³„ìˆ˜ (ì‚¬ìš©ìë³´ë‹¤ ì‘ê²Œ ì„¤ì •)
    num_epochs=50,
    num_samples_per_epoch=50000,
    top_k=10,
    eval_users=100,
    eval_interval=5     # í‰ê°€ ê°„ê²© (ì—í¬í¬)
):
    # ë°ì´í„°ì…‹ ë¡œë“œ
    dataset = BPRDataset(triplet_file)
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    num_users = dataset.metadata.get('num_users', 0)
    num_items = dataset.metadata.get('num_businesses', 0)
    model = BPRModel(num_users, num_items, embedding_dim)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ì‚¬ìš© ì¥ì¹˜: {device}")
    
    # LearnBPR ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ í•™ìŠµ
    model, losses, metrics_history = learn_bpr_algorithm(
        dataset=dataset,
        model=model,
        num_epochs=num_epochs,
        learning_rate=learning_rate,
        lambda_user=lambda_user,
        lambda_item=lambda_item,
        num_samples_per_epoch=num_samples_per_epoch,
        device=device,
        save_path=f"./bpr_model/original_bpr_e{embedding_dim}_lr{learning_rate}_lu{lambda_user}_li{lambda_item}.pt",
        eval_interval=eval_interval,
        eval_users=eval_users
    )
    
    # ëª¨ë¸ í‰ê°€
    metrics = evaluate_model(
        model=model,
        dataset=dataset,
        top_k=top_k,
        num_users=eval_users,
        device=device
    )
    
    return model, metrics, metrics_history


if __name__ == "__main__":
    print("BPR ë…¼ë¬¸ì— ì¶©ì‹¤í•œ êµ¬í˜„ - LearnBPR ì•Œê³ ë¦¬ì¦˜ í…ŒìŠ¤íŠ¸")
    
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
    params = {
        'triplet_file': "philly_bpr_triplets.json",
        'embedding_dim': 128,
        'learning_rate': 0.05,
        'lambda_user': 0.0025,
        'lambda_item': 0.00025,
        'num_epochs': 50,
        'num_samples_per_epoch': 50000,
        'top_k': 10,
        'eval_users': 100,
        'eval_interval': 5
    }
    
    # ì‹¤í–‰
    model, metrics, metrics_history = main(**params)
    
    # ë©”íŠ¸ë¦­ í‚¤ ë¯¸ë¦¬ ìƒì„±
    map_key = f"MAP@{params['top_k']}"
    recall_key = f"Recall@{params['top_k']}"
    ndcg_key = f"NDCG@{params['top_k']}"
    
    print("\ní•™ìŠµ ë° í‰ê°€ ì™„ë£Œ")
    print(f"ìµœì¢… MAP@{params['top_k']}: {metrics[map_key]:.4f}")
    print(f"ìµœì¢… Recall@{params['top_k']}: {metrics[recall_key]:.4f}")
    print(f"ìµœì¢… NDCG@{params['top_k']}: {metrics[ndcg_key]:.4f}") 